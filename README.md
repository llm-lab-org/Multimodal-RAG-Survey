# From Text to Multimodality: A Survey on Multimodal Retrieval-Augmented Generation

This repository is designed to collect and categorize papers related to Multimodal Retrieval-Augmented Generation (RAG) according to our survey paper: [From Text to Multimodality: A Survey on Multimodal Retrieval-Augmented Generation](https://arxiv.org/abs/xxxxx). Given the rapid growth in this field, we will continuously update both the paper and this repository to serve as a resource for researchers working on future projects.

## Abstract
Large Language Models (LLMs) struggle with hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external information, thereby enhancing factual grounding. Recent advances in Multimodal Learning have extended RAG to Multimodal RAG, incorporating multiple modalities such as text, images, audio, and video to improve generative models. However, cross-modal alignment and reasoning introduce unique challenges, distinguishing Multimodal RAG from its text-based counterpart and leaving room for improvement.
This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering methodologies, datasets, benchmarks, evaluation metrics, and innovations in retrieval, fusion, augmentation, and generation. We precisely review training strategies, robustness enhancements, and loss functions, while also exploring the diverse tasks and applications these systems support. 
Furthermore, we discuss open challenges and future research directions to support advancements in this evolving field. This survey provides a foundation for developing more capable and reliable AI systems by bridging the gap between unimodal RAG and multimodal generative systems.

![Figure]()


## Papers

## Citations
If you find our paper, code, data, or models useful, please cite the paper:
```

```

## Contact
If you have questions, please send an email to mahdi.abootorabi2@gmail.com or ...
